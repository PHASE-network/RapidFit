

In order to perform a 2D or FC on the grid the dataset has to be made available in the BookKeeping as >2011 data is much larger than 10M so we can't stick it in the sandbox as in 2010
(I'm working on making FC work with the result of a 2DLL but this hasn't been tested yet, but we can upload the result of a 2DLL scan as input in the sandbox of the grid job and use Tier2 in this instance)





To Upload your data to the grid (the easy way):


in ganga:
		pfn=PhysicalFile("/tmp/rcurrie/Pass3-version2_Bs_310711_RapidFit.root")
		pfn.upload('/lhcb/user/r/rcurrie/Pass3-version2_Bs_310711_RapidFit.root','CERN-USER')

to make the file available across the whole Tier1:


		SetupProject LHCbDirac
		lhcb-proxy-init
		for i in GRIDKA-USER NIKHEF-USER IN2P3-USER RAL-USER CNAF-USER; do dirac-dms-replicate-lfn LFN:/lhcb/user/r/rcurrie/Pass3-version2_Bs_310711_RapidFit.root $i CERN-USER; done


(I think in time the LFN will be replicated through all sites but lets just do it now to speed things up
note that the documentation for this tool states the arguments should be LFN: Source-SE Destination-SE,   This is INCORRECT )


in the LHCb computing model you can only access LFNs from Tier1
(this is the way that the VO is setup and is not likely to change any time soon, (AKA you can't escape this without leaving LHCbs VO)
to run on Tier2 (with access to more CPU) you need to remove your reliance on LFNs)







To Run a job on the grid using your XML:




0	login to lxplus and download RapidFit SVN




0a			SETUP  GANGA  &  ROOT

setup Ganga:
			SetupProject Ganga

(verify that the command:		'which root'		returns:	"/afs/cern.ch/sw/lcg/app/releases/ROOT/5.28.00b/x86_64-slc5-gcc43-opt/root/bin/root" )
(if it doesn't, now run:
					source /afs/cern.ch/sw/lcg/app/releases/ROOT/5.28.00b/x86_64-slc5-gcc43-opt/root/bin/thisroot.sh		)

(FYI:	if you compile against 5.26 you need to request 5.26 on the grid by editing the submittion script ROOT_VERSION )


1			COMPILE  RAPIDFIT

compile RapidFit as a ROOT library

run:
	make -j5 lib



2

edit FC_Scan.py to include the $RAPIDFITROOT path

i.e. edit the RapidFit_Path variable to point to the root of your svn checkout

2a
edit FC_Scan.py to determine how many toys at each point you want (recommended <=500, more than 500 toys per subjob may be safe but is untested)

edit the parameter:
			 FC_POINTS_PER_JOB		for toys per core
			 FC_LAYERS			for the number of layers to submit

running with more toys per grid job is quicker to submit and easier to manage (order 1,600 grid jobs) but this makes a grid failiure (there will be a few) much more harmful to statistics

running with multiple layers means grid failiures could be left to chance, but this means a LOT of grid jobs (order 8-10k) this will likely get you noticed and it as of this writing unadvised

the more jobs the longer ganga will appear to hang with a python process at 100% CPU
(it is doing something and it's going to create a LOT of files on AFS
		give ganga a few hundred Mb to play with for the job... again I'm not joking here, ganga is VERY VERY ill suited to this task

		my $HOME/gangadir grew to > 500Mb when I submitted a job with 8k subjobs )


(you can edit sqrt_jobs_per_core=1, however running more than one point per CPU hasn't been tested on the grid yet and I don't know what the memory requirement would be)



3
edit your XML to simply point to the file of interest
i.e.
	<FileName>Pass3-version2_Bs_310711_RapidFit.root</FileName>



4

4a( very useful to start a screen session here as submitting to Dirac via ganga takes forever, i.e. a few hours)

(I'm looking to replace this reliance on ganga with a python-Dirac script as ganga is very poorly suited to what we're trying to do here with subjobs ordering into the many thousands, we only need a self-submitting Python-Dirac-Script which can run unattended and an input-sandbox, which takes 30sec to compress)

submit your job to the grid:

eg:

	ganga FC_Scan.py your.XML LFN:/lhcb/user/r/rcurrie/Pass3-version2_Bs_310711_RapidFit.root 2011-input-file1.txt 2011-input-file2.txt


	if your job requires an LFN: include it in the command line arguments and they're added to the job dependencies

	if your job requires a small local file to be uploaded (PFN) give the relative path and it's correctly added to the input sandbox




4b

This will prompt you for some last input, select option 1 (submit to grid)



			MAKE NOTE OF THE GANGA JOB NUMBER,	YOU WILL NEED THIS LATER


(4c	for completeness

I almost have a version of this script running on Castor/ECDF, however on both systems the python 'os' module isn't available and the hostname has to be extracted from a socket library
If I finalise this after Lepton Photon I will upload it as a single user inteface for all systems which is very friendly)




5
Wait for jobs to complete



watch for job failiures at:	
				https://lhcbweb.pic.es/DIRAC/LHCb-Production/lhcb_user/jobs/JobMonitor/display

set the number of jobs per page to 1000 (bottom of the window)

1 grid job = 1 ganga subjob

you can resubmit a failed job(s) (jobs with red on the left) by ticking the box left of the jobID and selecting the reschedule button, (top-right)

(requires your grid certificate to be in your browser)



6
Edit the GetDiracJobIDFromGanga.py script and replace the jobnumber = xxx with the number of your ganga job from step 5

6a	run ganga GetDiracJobIDFromGanga.py

This will now create a jobnum.log file where "jobnum" is the number of the job from step 5

(this .log file contains the jobID of all subjobs on the DIRAC monitoring system)


7
Setup Dirac:
			SetupProject LHCbDirac
Setup a Grid Proxy:
			lhcb-proxy-init

7a
to download the output from the Dirac jobs			DO NOT USE GANGA


Why?:			It will not complete in a timely manner (>24hr and YES I'm serious!!!)

Solution:		Use a script I wrote to download the output from Dirac



to use:
			python GetDiracOutput.py $PWD/jobnum.log /tmp/$USER/jobnum

This downloads the outputdata and outputsandbox from each jobID in the file $PWD/jobnum.log (you have to provide the absolute path of this file)

The output is stored in the directory /tmp/$USER/jobnum


This script has been written with the intent you should run it in parallel in a bash window, i.e.


			python GetDiracOutput.py $PWD/jobnum.log /tmp/$USER/jobnum & python GetDiracOutput.py $PWD/jobnum.log /tmp/$USER/jobnum & python GetDiracOutput.py $PWD/jobnum.log /tmp/$USER/jobnum & python GetDiracOutput.py $PWD/jobnum.log /tmp/$USER/jobnum & python GetDiracOutput.py $PWD/jobnum.log /tmp/$USER/jobnum

This downloads the outputs in 5 threads which is much faster than in ganga (I generally run something of the order of 50 to download larger result sets)

More threads removes the bottleneck due to Dirac lookups, but does come at the cost of not being able to understand the output, but when you have several thousand files to download you just want to set it up and walk away


8
You may have some subjobs still running or have been resubmitted, to re-download the output ONLY from these subjobs remove the corresponding folder in /tmp/$USER/jobnum and re-run the GetDiracOutput.py script



9
Plot your result using either the original plotting tool or RapidPlot

(RapidPlot provides a little more output when running and is slightly faster)


